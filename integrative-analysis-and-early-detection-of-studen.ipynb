{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9985284,"sourceType":"datasetVersion","datasetId":6144877}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# =============================================================================\n# Step 1: Import Libraries and Load Dataset\n# =============================================================================\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/student-depression-dataset/Student Depression Dataset.csv')\n\n# Display the initial preview of the data\nprint(\"Initial Data Preview:\")\nprint(df.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T14:46:44.999081Z","iopub.execute_input":"2025-04-15T14:46:44.999855Z","iopub.status.idle":"2025-04-15T14:46:48.229465Z","shell.execute_reply.started":"2025-04-15T14:46:44.999823Z","shell.execute_reply":"2025-04-15T14:46:48.228593Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# =============================================================================\n# Step 2: Data Cleaning and Preprocessing\n# =============================================================================","metadata":{}},{"cell_type":"code","source":"# Convert appropriate columns to 'category' types\ndf['Gender'] = df['Gender'].astype('category')\ndf['City'] = df['City'].astype('category')\n\n# Rename the column for suicidal thoughts to a simpler name and convert it using a mapping\ndf.rename(columns={'Have you ever had suicidal thoughts ?': 'Suicidal_Thoughts'}, inplace=True)\ndf['Suicidal_Thoughts'] = df['Suicidal_Thoughts'].map({'Yes': 1, 'No': 0})\n\n# Convert Yes/No in 'Family History of Mental Illness' to binary\ndf['Family History of Mental Illness'] = df['Family History of Mental Illness'].map({'Yes': 1, 'No': 0})\n\n# ----- Clean \"Sleep Duration\" column -----\n# Remove any double quotes and single quotes, plus extra whitespace.\ndf['Sleep Duration'] = df['Sleep Duration'].str.replace('\"', '', regex=False)\\\n                                      .str.strip(\"'\").str.strip()\n\n# Debug: Print distinct cleaned values in Sleep Duration\nprint(\"\\nDistinct values in 'Sleep Duration' after cleaning:\")\nprint(df['Sleep Duration'].unique())\n\n# Convert textual sleep duration descriptions to numeric values using a mapping.\nsleep_mapping = {\n    'Less than 5 hours': 4.0,\n    '5-6 hours': 5.5,\n    '7-8 hours': 7.5,\n    'More than 8 hours': 9.0\n}\ndf['Sleep_Duration_Num'] = df['Sleep Duration'].replace(sleep_mapping)\n\n# For any entries that remain as \"Others\", convert them to NaN\ndf['Sleep_Duration_Num'] = df['Sleep_Duration_Num'].replace(\"Others\", np.nan)\n\n# Fill missing sleep duration values (from \"Others\" or other mapping issues) with the median.\nmedian_sleep = df['Sleep_Duration_Num'].median()\ndf['Sleep_Duration_Num'] = df['Sleep_Duration_Num'].fillna(median_sleep)\n\n# Explicitly convert Sleep_Duration_Num to float\ndf['Sleep_Duration_Num'] = df['Sleep_Duration_Num'].astype(float)\n\n# Convert other numeric columns to proper numeric types.\nnumeric_cols = ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', \n                'Study Satisfaction', 'Job Satisfaction', 'Work/Study Hours', \n                'Financial Stress', 'Depression']\nfor col in numeric_cols:\n    df[col] = pd.to_numeric(df[col], errors='coerce')\n\n# Also ensure 'Family History of Mental Illness' is numeric.\ndf['Family History of Mental Illness'] = pd.to_numeric(df['Family History of Mental Illness'], errors='coerce')\n\n# Display dataset information after cleaning\nprint(\"\\nDataset Info After Cleaning:\")\nprint(df.info())\nprint(\"\\nData Preview After Cleaning:\")\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T14:46:48.230565Z","iopub.execute_input":"2025-04-15T14:46:48.230938Z","iopub.status.idle":"2025-04-15T14:46:48.319900Z","shell.execute_reply.started":"2025-04-15T14:46:48.230910Z","shell.execute_reply":"2025-04-15T14:46:48.318932Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# =============================================================================\n# Pre-EDA Data Integrity Checks\n# =============================================================================","metadata":{}},{"cell_type":"code","source":"# 1. Check for missing values and verify data types\nprint(\"\\nMissing Values per Column:\")\nprint(df.isnull().sum())\n\nprint(\"\\nData Types:\")\nprint(df.dtypes)\n\n# 2. Summary statistics for numeric features\nprint(\"\\nSummary Statistics for Numeric Features:\")\nprint(df.describe())\n\n# 3. Distribution checks (histograms and boxplots)\nnumeric_columns = [\n    'Age', 'Academic Pressure', 'Work Pressure', 'CGPA', \n    'Study Satisfaction', 'Job Satisfaction', 'Sleep_Duration_Num', \n    'Work/Study Hours', 'Financial Stress', \n    'Family History of Mental Illness', 'Depression'\n]\n\nplt.figure(figsize=(12, 10))\nfor i, col in enumerate(numeric_columns):\n    plt.subplot(4, 3, i+1)\n    plt.hist(df[col].dropna(), bins=20, edgecolor='black')\n    plt.title(col)\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(12, 10))\nfor i, col in enumerate(numeric_columns):\n    plt.subplot(4, 3, i+1)\n    sns.boxplot(y=df[col])\n    plt.title(col)\nplt.tight_layout()\nplt.show()\n\n# 4. Preliminary Correlation Matrix\nplt.figure(figsize=(10, 8))\ncorr_pre = df[numeric_columns].corr()\nsns.heatmap(corr_pre, annot=True, cmap='coolwarm')\nplt.title('Preliminary Correlation Matrix')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T14:46:48.321783Z","iopub.execute_input":"2025-04-15T14:46:48.322275Z","iopub.status.idle":"2025-04-15T14:46:52.307121Z","shell.execute_reply.started":"2025-04-15T14:46:48.322252Z","shell.execute_reply":"2025-04-15T14:46:52.306252Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# =============================================================================\n# Step 3: Exploratory Data Analysis (EDA)\n# =============================================================================","metadata":{}},{"cell_type":"code","source":"# Univariate Analysis: Distribution of CGPA and Age\nplt.figure(figsize=(8, 4))\nplt.hist(df['CGPA'].dropna(), bins=20, edgecolor='black')\nplt.xlabel('CGPA')\nplt.ylabel('Frequency')\nplt.title('Distribution of CGPA')\nplt.show()\n\nplt.figure(figsize=(8, 4))\nplt.hist(df['Age'].dropna(), bins=15, edgecolor='black')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.title('Distribution of Age')\nplt.show()\n\n# Bivariate Analysis: Scatter plot of Academic Pressure vs. CGPA\nplt.figure(figsize=(8, 4))\nplt.scatter(df['Academic Pressure'], df['CGPA'])\nplt.xlabel('Academic Pressure')\nplt.ylabel('CGPA')\nplt.title('Academic Pressure vs. CGPA')\nplt.show()\n\n# Correlation Analysis: Visualize correlation among key numeric features\nnumeric_features_eda = [\n    'Age', 'Academic Pressure', 'Work Pressure', 'CGPA', \n    'Study Satisfaction', 'Job Satisfaction', 'Sleep_Duration_Num', \n    'Work/Study Hours', 'Financial Stress', \n    'Family History of Mental Illness', 'Depression'\n]\n\ncorr_matrix = df[numeric_features_eda].corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T14:46:52.308006Z","iopub.execute_input":"2025-04-15T14:46:52.308258Z","iopub.status.idle":"2025-04-15T14:46:53.408371Z","shell.execute_reply.started":"2025-04-15T14:46:52.308233Z","shell.execute_reply":"2025-04-15T14:46:53.407497Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# =============================================================================\n# Step 4: Data Synthesizing (Feature Engineering)\n# =============================================================================","metadata":{}},{"cell_type":"code","source":"# a. Create a \"Stress_Score\" by combining Academic Pressure and Work Pressure.\ndf['Stress_Score'] = (df['Academic Pressure'] + df['Work Pressure']) / 2\n\n# b. Synthesize a \"Lifestyle_Balance\" metric using Sleep Duration, Study Satisfaction, and Job Satisfaction.\nlifestyle_features = ['Sleep_Duration_Num', 'Study Satisfaction', 'Job Satisfaction']\n\nscaler = StandardScaler()\nlifestyle_scaled = scaler.fit_transform(df[lifestyle_features])\n\n# Create a DataFrame for scaled values with the same index as df.\nlifestyle_scaled_df = pd.DataFrame(\n    lifestyle_scaled,\n    columns=[f'{feat}_scaled' for feat in lifestyle_features],\n    index=df.index\n)\n\n# Concatenate the scaled DataFrame with the original DataFrame.\ndf = pd.concat([df, lifestyle_scaled_df], axis=1)\n\n# Create the composite Lifestyle Balance metric with adjusted weights:\ndf['Lifestyle_Balance'] = (\n    0.4 * df['Sleep_Duration_Num_scaled'] +\n    0.3 * df['Study Satisfaction_scaled'] +\n    0.3 * df['Job Satisfaction_scaled']\n)\n\nprint(\"\\nNew columns created in Step 4:\")\nprint(df[['Stress_Score', 'Lifestyle_Balance']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T14:46:53.409234Z","iopub.execute_input":"2025-04-15T14:46:53.409477Z","iopub.status.idle":"2025-04-15T14:46:53.444318Z","shell.execute_reply.started":"2025-04-15T14:46:53.409458Z","shell.execute_reply":"2025-04-15T14:46:53.443486Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# =============================================================================\n# Step 5: Advanced Analysis — Clustering and Cluster Profiling\n# =============================================================================","metadata":{}},{"cell_type":"code","source":"\n\n# Select features for clustering (including synthesized features)\ncluster_features = ['Age', 'CGPA', 'Sleep_Duration_Num', 'Stress_Score', 'Lifestyle_Balance']\n\n# Perform K-Means clustering\nkmeans = KMeans(n_clusters=3, random_state=42)\ndf['Cluster'] = kmeans.fit_predict(df[cluster_features])\n\n# Use PCA to reduce dimensions to 2 for visualization.\npca = PCA(n_components=2)\ncomponents = pca.fit_transform(df[cluster_features])\ndf['Component1'] = components[:, 0]\ndf['Component2'] = components[:, 1]\n\n# Plot the PCA scatter plot with clusters.\nplt.figure(figsize=(8, 6))\nscatter = plt.scatter(df['Component1'], df['Component2'], c=df['Cluster'], cmap='viridis', alpha=0.6)\nplt.xlabel('PCA Component 1')\nplt.ylabel('PCA Component 2')\nplt.title('Clusters Based on Synthesized & Adjusted Features')\nplt.colorbar(scatter, ticks=[0, 1, 2])\nplt.show()\n\n# =======================\n# Cluster Profiling\n# =======================\n# Compute descriptive statistics for each cluster (mean values)\ncluster_profile_mean = df.groupby('Cluster')[cluster_features].mean()\nprint(\"Cluster Profiling (Mean Values):\")\nprint(cluster_profile_mean)\n\n# Compute standard deviations for each cluster\ncluster_profile_std = df.groupby('Cluster')[cluster_features].std()\nprint(\"\\nCluster Profiling (Standard Deviations):\")\nprint(cluster_profile_std)\n\n# -----------------------\n# Domain-Specific Labeling\n# -----------------------\n# Based on printed outputs\n#   - Cluster 0 show high Stress_Score, moderate Sleep_Duration_Num, and high CGPA,\n#   - Cluster 1 show moderate Stress_Score, high Sleep_Duration_Num, and lower CGPA, and\n#   - Cluster 2 show lower Stress_Score, moderate Sleep_Duration_Num, and moderate CGPA,\ncluster_labels = {\n    0: \"High Stress / Moderate Sleep / High CGPA\",\n    1: \"Moderate Stress / High Sleep / Lower CGPA\",\n    2: \"Low Stress / Moderate Sleep / Moderate CGPA\"\n}\n\n# Map labels to a new column in DataFrame.\ndf['Cluster_Label'] = df['Cluster'].map(cluster_labels)\n\nprint(\"\\nSample with Cluster Labels:\")\nprint(df[['Cluster', 'Cluster_Label']].head())\n\n# Annotate the PCA scatter plot with the domain-specific labels.\nplt.figure(figsize=(8, 6))\nfor clus in sorted(df['Cluster'].unique()):\n    cluster_data = df[df['Cluster'] == clus]\n    plt.scatter(cluster_data['Component1'], cluster_data['Component2'],\n                label=cluster_labels.get(clus, f\"Cluster {clus}\"),\n                alpha=0.6)\nplt.xlabel('PCA Component 1')\nplt.ylabel('PCA Component 2')\nplt.title('Clusters with Domain-Specific Labels')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T14:52:07.328638Z","iopub.execute_input":"2025-04-15T14:52:07.328970Z","iopub.status.idle":"2025-04-15T14:52:08.606618Z","shell.execute_reply.started":"2025-04-15T14:52:07.328930Z","shell.execute_reply":"2025-04-15T14:52:08.605760Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# =============================================================================\n# Step 6: Predictive Modeling with Logistic Regression\n# =============================================================================","metadata":{}},{"cell_type":"code","source":"# Define predictor columns (original + synthesized features).\npredictor_cols = [\n    'Age', \n    'Academic Pressure', \n    'Work Pressure', \n    'CGPA', \n    'Study Satisfaction', \n    'Job Satisfaction', \n    'Sleep_Duration_Num', \n    'Stress_Score', \n    'Lifestyle_Balance', \n    'Work/Study Hours', \n    'Financial Stress', \n    'Family History of Mental Illness'\n]\n\nX = df[predictor_cols]\ny = df['Depression']  #(0 or 1)\n\n# Impute any remaining missing values in X using the median.\nX = X.fillna(X.median())\n\n# Split the dataset into training and testing sets.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Build and train the logistic regression model.\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Evaluate model performance.\nprint(\"Logistic Regression Model Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T14:46:54.795102Z","iopub.execute_input":"2025-04-15T14:46:54.795441Z","iopub.status.idle":"2025-04-15T14:46:55.443120Z","shell.execute_reply.started":"2025-04-15T14:46:54.795409Z","shell.execute_reply":"2025-04-15T14:46:55.441482Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n**In this analysis, a logistic regression model was developed to predict the binary outcome of student depression. The final model achieved an accuracy of approximately 78.6%, with class-specific performance indicating a precision of 80% and a recall of 84% for the depressed class.**\n\n**These results suggest that the model reliably identifies students at risk while maintaining a balanced performance across classes.Additionally, K-Means clustering was applied using key features (Age, CGPA, Sleep Duration, Stress Score, and Lifestyle Balance) and visualized via PCA, enabling the segmentation of the student population into three distinct groups. This clustering facilitates the identification of inherent patterns within the data that may inform targeted intervention strategies.**\n\n**Overall, the integration of unsupervised clustering with supervised logistic regression provides a complementary framework for both profiling and predictive analysis, offering valuable insights for early detection and potential intervention in student depression.**\n","metadata":{}}]}